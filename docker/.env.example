# duo-talk vLLM Docker Environment Configuration
# Copy this file to .env and adjust values as needed

# Hugging Face Token (required for some models)
HF_TOKEN=

# vLLM Model Configuration
# Default: RedHatAI/gemma-3-12b-it-quantized.w8a8 (INT8, ~14GB VRAM)
# Alternative: ISTA-DASLab/gemma-3-12b-it-GPTQ-4b-128g (GPTQ, ~7GB VRAM)
# Alternative: Qwen/Qwen2.5-14B-Instruct-AWQ (AWQ, ~10GB VRAM, no VLM)
VLLM_MODEL=RedHatAI/gemma-3-12b-it-quantized.w8a8

# GPU Memory Utilization (0.0-1.0)
# Higher = more VRAM for model, less for KV cache
GPU_MEMORY_UTIL=0.85

# Maximum Model Context Length
MAX_MODEL_LEN=8192

# Port Configuration
VLLM_PORT=8000
OLLAMA_PORT=11434

# Hugging Face Cache Directory
HF_CACHE=~/.cache/huggingface

# Extra vLLM Arguments (optional)
# Example: --quantization awq (for AWQ models)
VLLM_EXTRA_ARGS=
