#!/usr/bin/env python3
"""
Vision â†’ Character â†’ Director ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
è¦³å…‰åœ°ã®ç”»åƒãƒ»å‹•ç”»ã«å¯¾ã—ã¦ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»è§£èª¬ã‚’ç”Ÿæˆã—ã€å“è³ªåˆ¤å®šã™ã‚‹ã€‚
"""

import sys
import json
from pathlib import Path
from typing import Optional

sys.path.insert(0, str(Path(__file__).parent.parent))

from src.vision_processor import VisionProcessor
from src.character import Character
from src.director import Director
from src.logger import Logger


class NarrationPipeline:
    """
    Visionåˆ†æ â†’ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å¯¾è©±ç”Ÿæˆ â†’ Directorå“è³ªåˆ¤å®š
    """

    def __init__(self):
        self.vision_processor = VisionProcessor()
        self.char_a = Character("A")
        self.char_b = Character("B")
        self.director = Director()
        self.logger = Logger()

    def _emit_speak_event(
        self,
        run_id: str,
        turn: int,
        speaker: str,
        text: str,
        beat: Optional[str] = None,
    ) -> None:
        """GUIç”¨ã®speakã‚¤ãƒ™ãƒ³ãƒˆã‚’ç™ºè¡Œ"""
        from datetime import datetime
        self.logger.log_event({
            "event": "speak",
            "run_id": run_id,
            "turn": turn,
            "speaker": speaker,
            "text": text,
            "beat": beat,
            "ts": datetime.now().isoformat(),
        })

    def _emit_director_event(
        self,
        run_id: str,
        turn: int,
        beat: str,
        cut_cue: Optional[str] = None,
        status: Optional[str] = None,
        reason: Optional[str] = None,
    ) -> None:
        """GUIç”¨ã®directorã‚¤ãƒ™ãƒ³ãƒˆã‚’ç™ºè¡Œ"""
        from datetime import datetime
        self.logger.log_event({
            "event": "director",
            "run_id": run_id,
            "turn": turn,
            "beat": beat,
            "cut_cue": cut_cue,
            "status": status,
            "reason": reason,
            "ts": datetime.now().isoformat(),
        })

    def process_image(
        self,
        image_path: Optional[str],
        scene_description: str,
        max_iterations: int = 2,
        run_id: Optional[str] = None,
        skip_vision: bool = False,
    ) -> dict:
        """
        ç”»åƒã¾ãŸã¯ãƒˆãƒ”ãƒƒã‚¯ã«å¯¾ã—ã¦ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»è§£èª¬ã‚’ç”Ÿæˆã™ã‚‹ã€‚

        Args:
            image_path: å…¥åŠ›ç”»åƒã®ãƒ‘ã‚¹ï¼ˆskip_vision=True ã®å ´åˆã¯ä¸è¦ï¼‰
            scene_description: ã‚·ãƒ¼ãƒ³ã®èª¬æ˜ï¼ˆèª²é¡Œãƒ†ãƒ¼ãƒï¼‰
            max_iterations: ãƒªãƒˆãƒ©ã‚¤ã®æœ€å¤§å›æ•°
            run_id: GUIç”¨ã®ãƒ©ãƒ³ID
            skip_vision: Trueã®å ´åˆã€Visionåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ãƒˆãƒ”ãƒƒã‚¯ã®ã¿ã§å¯¾è©±ç”Ÿæˆ

        Returns:
            {
                "status": "success" | "skip" | "error",
                "scene_description": str,
                "image_path": str,
                "vision_analysis": dict,
                "dialogue": {
                    "char_a_turn_1": str,
                    "char_b_turn_1": str,
                    "char_a_turn_2": str,
                    "char_b_turn_2": str (optional),
                    ...
                },
                "director_verdict": dict,
                "log_id": str (optional)
            }
        """
        # run_id ãŒãªã‘ã‚Œã°ç”Ÿæˆ
        if run_id is None:
            from datetime import datetime
            run_id = f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        result = {
            "status": "processing",
            "scene_description": scene_description,
            "image_path": image_path,
            "vision_analysis": None,
            "dialogue": {},
            "director_verdict": None,
            "log_id": None,
            "run_id": run_id,
        }

        print(f"\n{'='*60}")
        print(f"ğŸ“· Topic: {scene_description}")
        if not skip_vision and image_path:
            print(f"ğŸ–¼ï¸  Image: {image_path}")
        print(f"ğŸ†” Run ID: {run_id}")
        print(f"{'='*60}")

        # Step 1: Vision åˆ†æï¼ˆskip_vision=True ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        vision_text = None
        if skip_vision or not image_path:
            print("\n[Step 1] Skipping Vision analysis (topic-only mode)")
            result["vision_analysis"] = {"status": "skipped", "reason": "topic-only mode"}
        else:
            print("\n[Step 1] Analyzing image with Vision LLM...")
            vision_result = self.vision_processor.analyze_image(image_path)

            if vision_result["status"] == "error":
                print(f"âŒ Vision analysis failed: {vision_result.get('error')}")
                result["status"] = "error"
                result["vision_analysis"] = vision_result
                return result

            print("âœ… Vision analysis complete")
            result["vision_analysis"] = vision_result

            # Vision æƒ…å ±ã‚’ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
            vision_text = self.vision_processor.format_for_character(
                vision_result["visual_info"]
            )

        # Step 2: ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å¯¾è©±ç”Ÿæˆ
        # max_iterations = å¯¾è©±ã‚¿ãƒ¼ãƒ³æ•°ï¼ˆAâ†’Bâ†’Aâ†’B...ï¼‰
        print("\n[Step 2] Generating character dialogue...")

        dialogue_history = []
        turn_counter = 0

        # A ãŒåˆæ‰‹ã‚’æ‰“ã¤
        print(f"\n  Turn {turn_counter + 1}/{max_iterations}")
        print("    > æ¾„ãƒ¶ç€¬ã‚„ãª (å§‰) is speaking...")
        char_a_speech = self.char_a.speak(
            frame_description=scene_description,
            vision_info=vision_text,
        )
        print(f"      {char_a_speech}")
        result["dialogue"][f"turn_{turn_counter}"] = {"speaker": "A", "text": char_a_speech}
        dialogue_history.append(("A", char_a_speech))
        self._emit_speak_event(run_id, turn_counter, "A", char_a_speech)
        turn_counter += 1

        # æ®‹ã‚Šã®ã‚¿ãƒ¼ãƒ³ã‚’äº¤äº’ã«ç”Ÿæˆ
        for turn in range(1, max_iterations):
            print(f"\n  Turn {turn + 1}/{max_iterations}")

            # å‰ã®ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ã‚’å–å¾—
            last_speaker, last_speech = dialogue_history[-1]

            # æ¬¡ã®ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ã‚’æ±ºå®šï¼ˆäº¤äº’ï¼‰
            if last_speaker == "A":
                current_speaker = "B"
                current_char = self.char_b
                speaker_name = "æ¾„ãƒ¶ç€¬ã‚ã‚† (å¦¹)"
            else:
                current_speaker = "A"
                current_char = self.char_a
                speaker_name = "æ¾„ãƒ¶ç€¬ã‚„ãª (å§‰)"

            # ç™ºè¨€ç”Ÿæˆ
            print(f"    > {speaker_name} is speaking...")
            speech = current_char.speak(
                frame_description=scene_description,
                partner_speech=last_speech,
                vision_info=vision_text,
            )
            print(f"      {speech}")
            result["dialogue"][f"turn_{turn_counter}"] = {"speaker": current_speaker, "text": speech}
            dialogue_history.append((current_speaker, speech))
            self._emit_speak_event(run_id, turn_counter, current_speaker, speech)

            # Director ã«ã‚ˆã‚‹å“è³ªåˆ¤å®šï¼ˆæ¯ã‚¿ãƒ¼ãƒ³ï¼‰
            print(f"    > Director is judging...")
            previous_speech = dialogue_history[-2][1] if len(dialogue_history) > 1 else None

            director_evaluation = self.director.evaluate_response(
                frame_description=scene_description,
                speaker=current_speaker,
                response=speech,
                partner_previous_speech=previous_speech,
                speaker_domains=current_char.domains,
            )

            # beat ã‚’æ±ºå®š
            beat_map = {"PASS": "PAYOFF", "RETRY": "BANter", "MODIFY": "PIVOT"}
            beat = beat_map.get(director_evaluation.status.name, "BANter")

            # GUIç”¨ director ã‚¤ãƒ™ãƒ³ãƒˆã‚’ç™ºè¡Œ
            self._emit_director_event(
                run_id,
                turn_counter,
                beat,
                director_evaluation.suggestion,
                status=director_evaluation.status.name,
                reason=director_evaluation.reason,
            )

            print(f"      [{director_evaluation.status.name}] {director_evaluation.reason}")

            # æœ€çµ‚ã‚¿ãƒ¼ãƒ³ã®å ´åˆã®ã¿ verdict ã‚’è¨˜éŒ²
            if turn == max_iterations - 1:
                result["director_verdict"] = {
                    "status": str(director_evaluation.status.name),
                    "reason": director_evaluation.reason,
                    "suggestion": director_evaluation.suggestion,
                }

            turn_counter += 1

            # MODIFY ã®å ´åˆã¯æ—©æœŸçµ‚äº†
            if director_evaluation.status.name == "MODIFY":
                print("\nâš ï¸  Director requested modification. Ending dialogue.")
                result["status"] = "skip"
                break
        else:
            # ãƒ«ãƒ¼ãƒ—ãŒæ­£å¸¸å®Œäº†ã—ãŸå ´åˆ
            print(f"\nâœ… Dialogue completed ({turn_counter} turns)")
            result["status"] = "success"

        # Step 4: ãƒ­ã‚°ã«è¨˜éŒ²
        print("\n[Step 3] Logging to file...")
        log_id = self.logger.log_narration(
            scene_description=scene_description,
            image_path=image_path,
            vision_analysis=result["vision_analysis"],
            dialogue=result["dialogue"],
            director_verdict=result["director_verdict"],
        )
        result["log_id"] = log_id
        print(f"âœ… Logged (ID: {log_id})")

        return result

    def process_batch(
        self,
        image_list: list,
        output_file: Optional[str] = None,
    ) -> list:
        """
        è¤‡æ•°ã®ç”»åƒã‚’å‡¦ç†ã™ã‚‹ã€‚

        Args:
            image_list: [(image_path, scene_description), ...] ã®ãƒªã‚¹ãƒˆ
            output_file: çµæœã‚’JSONã§å‡ºåŠ›ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆoptionalï¼‰

        Returns:
            çµæœã®ãƒªã‚¹ãƒˆ
        """
        results = []

        for image_path, scene_description in image_list:
            result = self.process_image(
                image_path=image_path,
                scene_description=scene_description,
            )
            results.append(result)

        # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆæŒ‡å®šæ™‚ï¼‰
        if output_file:
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ“ Results saved to: {output_file}")

        return results


def main():
    """
    ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œä¾‹
    """
    pipeline = NarrationPipeline()

    # ãƒ†ã‚¹ãƒˆç”¨ã®ç”»åƒãƒªã‚¹ãƒˆ
    test_images = [
        ("tests/images/temple_sample.jpg", "å¤ã„å¯ºé™¢ã®å¢ƒå†…ã€‚å‚æ‹å®¢ãŒå°‘ãªãã€é™ã‹ãªæ™‚é–“å¸¯ã®ã‚ˆã†ã§ã™ã€‚"),
        ("tests/images/nature_sample.jpg", "ç·‘è±Šã‹ãªå±±é–“ã®é¢¨æ™¯ã€‚è‡ªç„¶ã«åŒ…ã¾ã‚ŒãŸè¦³å…‰åœ°ã§ã™ã€‚"),
    ]

    if not test_images:
        print("No test images provided.")
        print("Usage: python scripts/run_narration.py")
        print("\nTo test:")
        print("1. ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«é…ç½®")
        print("2. ä»¥ä¸‹ã‚’ã‚³ãƒ¼ãƒ‰å†…ã§è¨­å®šï¼š")
        print('   test_images = [("path/to/image.jpg", "ã‚·ãƒ¼ãƒ³èª¬æ˜"), ...]')
        print("3. ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ")
        return

    results = pipeline.process_batch(
        image_list=test_images,
        output_file="runs/narration_results.json",
    )

    print("\n" + "="*60)
    print(f"âœ… Processing complete! ({len([r for r in results if r['status'] == 'success'])} / {len(results)} passed)")
    print("="*60)


if __name__ == "__main__":
    main()
