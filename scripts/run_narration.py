#!/usr/bin/env python3
"""
Vision â†’ Character â†’ Director ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
è¦³å…‰åœ°ã®ç”»åƒãƒ»å‹•ç”»ã«å¯¾ã—ã¦ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»è§£èª¬ã‚’ç”Ÿæˆã—ã€å“è³ªåˆ¤å®šã™ã‚‹ã€‚
"""

import sys
import json
import re
import warnings
from pathlib import Path
from typing import Optional, List, Tuple, Dict, Any
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent.parent))

from src.vision_processor import VisionProcessor
from src.character import Character
from src.director import Director
from src.logger import Logger
from src.owner_intervention import get_intervention_manager, InterventionState
from src.unified_pipeline import UnifiedPipeline, DialogueResult
from src.input_source import InputBundle, InputSource, SourceType


class NarrationPipeline:
    """
    Visionåˆ†æ â†’ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å¯¾è©±ç”Ÿæˆ â†’ Directorå“è³ªåˆ¤å®š
    """

    # ãƒªãƒˆãƒ©ã‚¤äºˆç®—: 1ã‚¿ãƒ¼ãƒ³ã‚ãŸã‚Šã®æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
    MAX_RETRY_PER_TURN = 1

    # å®¶æ—è¨­å®šï¼ˆå…¨ã‚·ãƒ¼ãƒ³ã«å…±é€šï¼‰
    FAMILY_CONTEXT = "ã€å‰æã€‘ã‚„ãªã¨ã‚ã‚†ã¯å§‰å¦¹ã§ã€åŒã˜å®¶ã«ä½ã‚“ã§ã„ã¾ã™ã€‚è¦ªæˆšãƒ»å®Ÿå®¶ã¸ã®è¨ªå•ã¯ä¸€ç·’ã«è¡Œãå‰æã§ã™ã€‚"

    # ãƒˆãƒ”ãƒƒã‚¯åˆ¥ã®å…·ä½“çš„ãªã‚·ãƒ¼ãƒ³ãƒ’ãƒ³ãƒˆ
    TOPIC_HINTS = {
        "ãŠæ­£æœˆ": "ã“ãŸã¤ã§äºˆå®šè¡¨ã‚’ä½œã‚ŠãªãŒã‚‰ã€‚è©±é¡Œå€™è£œï¼šåˆè©£ã€ãŠã¿ãã˜ã€é›‘ç…®ã®åœ°åŸŸå·®ã€ç¦è¢‹ã€æ›¸ãåˆã‚ã€ç®±æ ¹é§…ä¼ã€å¹´è³€çŠ¶",
        "ã‚¯ãƒªã‚¹ãƒã‚¹": "ãƒªãƒ“ãƒ³ã‚°ã§ãƒ„ãƒªãƒ¼ã‚’çœºã‚ãªãŒã‚‰ã€‚è©±é¡Œå€™è£œï¼šãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆã€ã‚±ãƒ¼ã‚­ã®ç¨®é¡ã€ã‚¤ãƒ«ãƒŸãƒãƒ¼ã‚·ãƒ§ãƒ³ã€ã‚µãƒ³ã‚¿ã®ç”±æ¥",
        "èŠ±è¦‹": "æ¡œã®æœ¨ã®ä¸‹ã§ãŠå¼å½“ã‚’åºƒã’ãªãŒã‚‰ã€‚è©±é¡Œå€™è£œï¼šæ¡œã®å“ç¨®ã€å ´æ‰€å–ã‚Šã€èŠ±è¦‹å›£å­ã€å¤œæ¡œ",
        "å¤ç¥­ã‚Š": "æµ´è¡£ã‚’ç€ã¦å±‹å°ã‚’æ­©ããªãŒã‚‰ã€‚è©±é¡Œå€™è£œï¼šé‡‘é­šã™ãã„ã€ç¶¿ã‚ã‚ã€èŠ±ç«ã€ç›†è¸Šã‚Šã€ã‹ãæ°·",
        "ãŠç›†": "ä»å£‡ã®å‰ã§ã€‚è©±é¡Œå€™è£œï¼šãŠå¢“å‚ã‚Šã€ç²¾éœŠé¦¬ã€ç›†è¸Šã‚Šã€è¿ãˆç«é€ã‚Šç«ã€ãƒŠã‚¹ã¨ã‚­ãƒ¥ã‚¦ãƒª",
        "default": "ãƒªãƒ“ãƒ³ã‚°ã§ä¸€ç·’ã«è©±ã—ã¦ã„ã‚‹",
    }

    def __init__(self):
        self.vision_processor = VisionProcessor()
        self.logger = Logger()
        self.intervention_manager = get_intervention_manager()

        # UnifiedPipeline ã‚’å†…éƒ¨ã§ä½¿ç”¨
        self.unified_pipeline = UnifiedPipeline(
            jetracer_client=None,  # NarrationPipelineã§ã¯JetRacerä¸ä½¿ç”¨
            enable_fact_check=True,
        )

        # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ Character, Director ã¸ã®å‚ç…§ã‚’ç¶­æŒ
        self.char_a = self.unified_pipeline.char_a
        self.char_b = self.unified_pipeline.char_b
        self.director = self.unified_pipeline.director

        # Reset intervention state to RUNNING for new pipeline
        if self.intervention_manager.get_state() != InterventionState.RUNNING:
            print(f"[NarrationPipeline] Resetting intervention state from {self.intervention_manager.get_state().value} to RUNNING")
            self.intervention_manager.state = InterventionState.RUNNING
            self.intervention_manager.current_session = None

    def _wait_for_intervention(self, run_id: str, timeout: int = 60) -> Optional[str]:
        """
        ä»‹å…¥çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€ä¸€æ™‚åœæ­¢ä¸­ã¯å¾…æ©Ÿã™ã‚‹ã€‚

        Args:
            run_id: ç¾åœ¨ã®ãƒ©ãƒ³ID
            timeout: æœ€å¤§å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ60ç§’

        Returns:
            ã‚ªãƒ¼ãƒŠãƒ¼æŒ‡ç¤ºãŒã‚ã‚‹å ´åˆã¯ãã®å†…å®¹ã€ãªã‘ã‚Œã°None
        """
        import time

        start_time = time.time()

        while True:
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯
            elapsed = time.time() - start_time
            if elapsed > timeout:
                print(f"    âš ï¸  Intervention wait timeout ({timeout}s). Force resuming...")
                # å¼·åˆ¶çš„ã«RUNNINGçŠ¶æ…‹ã«æˆ»ã™
                self.intervention_manager.state = InterventionState.RUNNING
                self.intervention_manager.current_session = None
                return None

            state = self.intervention_manager.get_state()

            if state == InterventionState.RUNNING:
                # å®Ÿè¡Œä¸­ï¼šã‚ªãƒ¼ãƒŠãƒ¼æŒ‡ç¤ºãŒã‚ã‚Œã°å–å¾—
                instruction = self.intervention_manager.get_pending_instruction()
                if instruction:
                    # æŒ‡ç¤ºã‚’ã‚¯ãƒªã‚¢ï¼ˆä¸€åº¦ã ã‘é©ç”¨ï¼‰
                    self.intervention_manager.clear_pending_instruction()
                return instruction

            elif state in [InterventionState.PAUSED, InterventionState.PROCESSING,
                          InterventionState.QUERY_BACK]:
                # ä¸€æ™‚åœæ­¢ä¸­ï¼šå¾…æ©Ÿ
                print(f"    â¸ï¸  Paused by owner intervention (state: {state.value}, elapsed: {elapsed:.1f}s)")
                time.sleep(1.0)  # 1ç§’ã”ã¨ã«ãƒã‚§ãƒƒã‚¯

            elif state == InterventionState.RESUMING:
                # å†é–‹ä¸­ï¼šæŒ‡ç¤ºã‚’å–å¾—ã—ã¦å®Ÿè¡Œå†é–‹
                instruction = self.intervention_manager.get_pending_instruction()
                if instruction:
                    self.intervention_manager.clear_pending_instruction()
                return instruction

            else:
                # ä¸æ˜ãªçŠ¶æ…‹ã¯å®Ÿè¡Œç¶™ç¶š
                return None

    @staticmethod
    def _truncate_response(response: str, max_sentences: int = 2, max_chars: int = 100) -> str:
        """
        å¿œç­”ã‚’å¼·åˆ¶çš„ã«çŸ­ç¸®ã™ã‚‹ï¼ˆæ•£æ¼«æ¤œå‡ºã®æœ€å¾Œã®æ‰‹æ®µï¼‰ã€‚

        Args:
            response: å…ƒã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
            max_sentences: æœ€å¤§æ–‡æ•°
            max_chars: æœ€å¤§æ–‡å­—æ•°

        Returns:
            çŸ­ç¸®ã•ã‚ŒãŸå¿œç­”
        """
        # æ–‡æœ«è¨˜å·ã§åˆ†å‰²ï¼ˆè¨˜å·ã‚’ä¿æŒï¼‰
        parts = re.split(r'([ã€‚ï¼ï¼Ÿ])', response)
        result = ""
        sentence_count = 0

        # 2ã¤ãšã¤ãƒšã‚¢ï¼ˆæœ¬æ–‡ + å¥ç‚¹ï¼‰ã§å‡¦ç†
        i = 0
        while i < len(parts) - 1:
            if sentence_count >= max_sentences or len(result) > max_chars:
                break
            result += parts[i] + parts[i + 1]
            sentence_count += 1
            i += 2

        return result.strip() if result else response[:max_chars]

    def _generate_scene_description(self, base_scene: str) -> str:
        """
        ã‚·ãƒ¼ãƒ³èª¬æ˜ã«å®¶æ—è¨­å®šã¨å…·ä½“çš„ãªãƒ’ãƒ³ãƒˆã‚’è‡ªå‹•ä»˜ä¸ã™ã‚‹ã€‚

        Args:
            base_scene: åŸºæœ¬ã®ã‚·ãƒ¼ãƒ³èª¬æ˜

        Returns:
            å®¶æ—è¨­å®šã¨ãƒ’ãƒ³ãƒˆã‚’å«ã‚€å®Œå…¨ãªã‚·ãƒ¼ãƒ³èª¬æ˜
        """
        # ãƒˆãƒ”ãƒƒã‚¯ã«å¿œã˜ãŸãƒ’ãƒ³ãƒˆã‚’é¸æŠ
        hint = self.TOPIC_HINTS["default"]
        for key, value in self.TOPIC_HINTS.items():
            if key in base_scene:
                hint = value
                break

        return f"""ã€ã‚·ãƒ¼ãƒ³ã€‘{base_scene}
ã€çŠ¶æ³ã€‘å§‰å¦¹ã¯åŒã˜å®¶ã§ã€{hint}
ã€å‰æã€‘ã‚„ãªã¨ã‚ã‚†ã¯å§‰å¦¹ã§åŒå±…ã€‚ä¸€ç·’ã«éã”ã™å‰æã§ä¼šè©±ã™ã‚‹ã€‚
ã€é‡è¦ã€‘åŒã˜è©±é¡Œï¼ˆãŠã›ã¡ã€ãŠå¹´ç‰ç­‰ï¼‰ã‚’3å›ä»¥ä¸Šç¹°ã‚Šè¿”ã•ãªã„ã€‚æ–°ã—ã„è¦–ç‚¹ã‚„è©±é¡Œã‚’è¿½åŠ ã™ã‚‹ã€‚"""

    def _emit_speak_event(
        self,
        run_id: str,
        turn: int,
        speaker: str,
        text: str,
        beat: Optional[str] = None,
    ) -> None:
        """GUIç”¨ã®speakã‚¤ãƒ™ãƒ³ãƒˆã‚’ç™ºè¡Œ"""
        from datetime import datetime
        self.logger.log_event({
            "event": "speak",
            "run_id": run_id,
            "turn": turn,
            "speaker": speaker,
            "text": text,
            "beat": beat,
            "ts": datetime.now().isoformat(),
        })

    def _emit_director_event(
        self,
        run_id: str,
        turn: int,
        beat: str,
        cut_cue: Optional[str] = None,
        status: Optional[str] = None,
        reason: Optional[str] = None,
        guidance: Optional[str] = None,
        action: Optional[str] = None,
        hook: Optional[str] = None,
        evidence: Optional[dict] = None,
        # Director v3: Topic Manager fields
        focus_hook: Optional[str] = None,
        hook_depth: int = 0,
        depth_step: str = "DISCOVER",
        forbidden_topics: Optional[list] = None,
    ) -> None:
        """GUIç”¨ã®directorã‚¤ãƒ™ãƒ³ãƒˆã‚’ç™ºè¡Œ"""
        from datetime import datetime
        self.logger.log_event({
            "event": "director",
            "run_id": run_id,
            "turn": turn,
            "beat": beat,
            "cut_cue": cut_cue,
            "status": status,
            "reason": reason,
            "guidance": guidance,  # æ¬¡ã‚¿ãƒ¼ãƒ³ã¸ã®æŒ‡ç¤º
            "action": action,  # "NOOP" or "INTERVENE"
            "hook": hook,  # ä»‹å…¥ãƒˆãƒªã‚¬ãƒ¼ã¨ãªã‚‹å…·ä½“åè©
            "evidence": evidence,  # {"dialogue": ..., "frame": ...}
            # Director v3: Topic Manager fields
            "focus_hook": focus_hook,
            "hook_depth": hook_depth,
            "depth_step": depth_step,
            "forbidden_topics": forbidden_topics or [],
            "ts": datetime.now().isoformat(),
        })

    def _emit_rag_event(
        self,
        run_id: str,
        turn: int,
        char_id: str,
        rag_hints: list,
    ) -> None:
        """GUIç”¨ã®RAGé¸æŠã‚¤ãƒ™ãƒ³ãƒˆã‚’ç™ºè¡Œ"""
        from datetime import datetime

        # RAGãƒ’ãƒ³ãƒˆã‚’ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«æ•´ç†
        canon_preview = ""
        lore_preview = ""
        pattern_preview = ""

        for hint in rag_hints:
            if hint.startswith("["):
                # [domain] content ã®å½¢å¼
                bracket_end = hint.find("]")
                if bracket_end > 0:
                    domain = hint[1:bracket_end].lower()
                    content = hint[bracket_end+1:].strip()[:100]  # æœ€åˆã®100æ–‡å­—

                    if domain in ["sake", "tourism_aesthetics", "cultural_philosophy"]:
                        canon_preview = content
                    elif domain in ["geography", "history", "architecture"]:
                        lore_preview = content
                    else:
                        pattern_preview = content

        self.logger.log_event({
            "event": "rag_select",
            "run_id": run_id,
            "turn": turn,
            "char_id": char_id,
            "canon": {"preview": canon_preview},
            "lore": {"preview": lore_preview},
            "pattern": {"preview": pattern_preview},
            "ts": datetime.now().isoformat(),
        })

    def _build_conversation_context(
        self,
        dialogue_history: list,
        max_turns: int = 3,
    ) -> Optional[str]:
        """
        ç›´è¿‘ã®å¯¾è©±å±¥æ­´ã‹ã‚‰æ–‡è„ˆã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

        @deprecated: UnifiedPipeline ãŒå†…éƒ¨ã§å±¥æ­´ç®¡ç†ã™ã‚‹ãŸã‚ã€ç›´æ¥å‘¼ã³å‡ºã—ä¸è¦ã€‚
                     å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã«æ®‹ã—ã¦ã„ã‚‹ãŒã€æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ã§ã¯ä½¿ç”¨ã—ãªã„ã“ã¨ã€‚

        Args:
            dialogue_history: [(speaker, text), ...] ã®ãƒªã‚¹ãƒˆ
            max_turns: å«ã‚ã‚‹æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°

        Returns:
            ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸæ–‡è„ˆæ–‡å­—åˆ—ã€ã¾ãŸã¯å±¥æ­´ãŒãªã„å ´åˆã¯None
        """
        warnings.warn(
            "_build_conversation_context is deprecated. "
            "UnifiedPipeline handles conversation history internally.",
            DeprecationWarning,
            stacklevel=2
        )

        if not dialogue_history:
            return None

        # ç›´è¿‘ã®max_turnsåˆ†ã‚’å–å¾—
        recent = dialogue_history[-max_turns:]

        if len(recent) <= 1:
            return None  # ç›´è¿‘1ã‚¿ãƒ¼ãƒ³ã®ã¿ã®å ´åˆã¯æ–‡è„ˆä¸è¦

        lines = []
        for speaker, text in recent[:-1]:  # æœ€å¾Œã®ç™ºè¨€ã¯é™¤ãï¼ˆpartner_speechã§æ¸¡ã•ã‚Œã‚‹ãŸã‚ï¼‰
            char_name = "ã‚„ãª" if speaker == "A" else "ã‚ã‚†"
            lines.append(f"{char_name}: {text}")

        return "\n".join(lines) if lines else None

    def process_image(
        self,
        image_path: Optional[str],
        scene_description: str,
        max_iterations: int = 2,
        run_id: Optional[str] = None,
        skip_vision: bool = False,
        use_stateful_history: bool = True,  # ã“ã®å¼•æ•°ã¯ç„¡è¦–ï¼ˆå¸¸ã«statefulï¼‰
    ) -> dict:
        """
        ç”»åƒã¾ãŸã¯ãƒˆãƒ”ãƒƒã‚¯ã«å¯¾ã—ã¦ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»è§£èª¬ã‚’ç”Ÿæˆã™ã‚‹ã€‚

        å†…éƒ¨ã§ UnifiedPipeline ã‚’ä½¿ç”¨ã™ã‚‹ãŒã€æˆ»ã‚Šå€¤ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯æ—¢å­˜ã‚’ç¶­æŒã€‚

        Args:
            image_path: å…¥åŠ›ç”»åƒã®ãƒ‘ã‚¹ï¼ˆskip_vision=True ã®å ´åˆã¯ä¸è¦ï¼‰
            scene_description: ã‚·ãƒ¼ãƒ³ã®èª¬æ˜ï¼ˆèª²é¡Œãƒ†ãƒ¼ãƒï¼‰
            max_iterations: å¯¾è©±ã‚¿ãƒ¼ãƒ³æ•°
            run_id: GUIç”¨ã®ãƒ©ãƒ³ID
            skip_vision: Trueã®å ´åˆã€Visionåˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ãƒˆãƒ”ãƒƒã‚¯ã®ã¿ã§å¯¾è©±ç”Ÿæˆ
            use_stateful_history: å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™ãŒã€å¸¸ã«Trueæ‰±ã„

        Returns:
            {
                "status": "success" | "skip" | "error",
                "scene_description": str,
                "image_path": str,
                "vision_analysis": dict,
                "dialogue": {
                    "turn_0": {"speaker": "A", "text": str},
                    "turn_1": {"speaker": "B", "text": str},
                    ...
                },
                "director_verdict": dict,
                "log_id": str (optional)
            }
        """
        # run_id ãŒãªã‘ã‚Œã°ç”Ÿæˆ
        if run_id is None:
            run_id = f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        result = {
            "status": "processing",
            "scene_description": scene_description,
            "image_path": image_path,
            "vision_analysis": None,
            "dialogue": {},
            "director_verdict": None,
            "log_id": None,
            "run_id": run_id,
        }

        print(f"\n{'='*60}")
        print(f"ğŸ“· Topic: {scene_description or '(ç”»åƒã‹ã‚‰è‡ªå‹•ç”Ÿæˆ)'}")
        if not skip_vision and image_path:
            print(f"ğŸ–¼ï¸  Image: {image_path}")
        print(f"ğŸ†” Run ID: {run_id}")

        # Debug: Show intervention state at start
        intervention_state = self.intervention_manager.get_state()
        print(f"ğŸ”§ Intervention State: {intervention_state.value}")
        if intervention_state != InterventionState.RUNNING:
            print(f"âš ï¸  WARNING: Intervention state is not RUNNING, forcing reset...")
            self.intervention_manager.state = InterventionState.RUNNING
            self.intervention_manager.current_session = None

        print(f"{'='*60}")

        # Step 1: Vision åˆ†æï¼ˆskip_vision=True ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        vision_text = None
        # ã‚·ãƒ¼ãƒ³èª¬æ˜ã«å®¶æ—è¨­å®šã‚’è‡ªå‹•ä»˜ä¸
        effective_scene = self._generate_scene_description(scene_description) if scene_description else None

        if skip_vision or not image_path:
            print("\n[Step 1] Skipping Vision analysis (topic-only mode)")
            result["vision_analysis"] = {"status": "skipped", "reason": "topic-only mode"}
            # ãƒˆãƒ”ãƒƒã‚¯ã®ã¿ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€scene_descriptionãŒå¿…é ˆ
            if not effective_scene:
                effective_scene = self._generate_scene_description("è¦³å…‰åœ°ã‚’è¨ªã‚Œã¦ã„ã‚‹å ´é¢")
        else:
            print("\n[Step 1] Analyzing image with Vision LLM...")
            vision_result = self.vision_processor.analyze_image(image_path)

            if vision_result["status"] == "error":
                print(f"âŒ Vision analysis failed: {vision_result.get('error')}")
                result["status"] = "error"
                result["vision_analysis"] = vision_result
                return result

            print("âœ… Vision analysis complete")
            result["vision_analysis"] = vision_result

            # Vision æƒ…å ±ã‚’ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
            vision_text = self.vision_processor.format_for_character(
                vision_result["visual_info"]
            )

            # ãƒˆãƒ”ãƒƒã‚¯ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã€Visionåˆ†æçµæœã‹ã‚‰ã‚·ãƒ¼ãƒ³èª¬æ˜ã‚’ç”Ÿæˆ
            if not effective_scene:
                visual_info = vision_result.get("visual_info", {})
                main_subjects = visual_info.get("main_subjects", "")
                environment = visual_info.get("environment", "")

                # ãƒ¡ã‚¤ãƒ³è¢«å†™ä½“ã¨ç’°å¢ƒã‹ã‚‰ã‚·ãƒ¼ãƒ³èª¬æ˜ã‚’æ§‹ç¯‰
                if main_subjects:
                    base_scene = main_subjects
                    if environment:
                        base_scene = f"{main_subjects}ã€‚{environment}"
                elif environment:
                    base_scene = environment
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: raw_textã®æœ€åˆã®éƒ¨åˆ†ã‚’ä½¿ç”¨
                    raw_text = vision_result.get("raw_text", "")
                    base_scene = raw_text[:100] if raw_text else "ç”»åƒã«æ˜ ã‚‹é¢¨æ™¯"

                # å®¶æ—è¨­å®šã‚’ä»˜ä¸
                effective_scene = self._generate_scene_description(base_scene)
                print(f"ğŸ“ Generated scene from image: {base_scene[:50]}...")

        # Step 2: UnifiedPipeline ã§å¯¾è©±ç”Ÿæˆ
        print("\n[Step 2] Generating character dialogue with UnifiedPipeline...")

        # InputBundle ã‚’æ§‹ç¯‰
        sources = [InputSource(source_type=SourceType.TEXT, content=effective_scene)]
        if image_path and not skip_vision:
            sources.append(InputSource(source_type=SourceType.IMAGE_FILE, content=image_path))

        bundle = InputBundle(sources=sources)

        # å‰²ã‚Šè¾¼ã¿ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆã‚ªãƒ¼ãƒŠãƒ¼ä»‹å…¥å¯¾å¿œï¼‰
        def interrupt_callback() -> Optional[InputBundle]:
            instruction = self._wait_for_intervention(run_id)
            if instruction:
                # ã‚ªãƒ¼ãƒŠãƒ¼æŒ‡ç¤ºã‚’ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã¨ã—ã¦è¿”ã™
                return InputBundle(
                    sources=[InputSource(source_type=SourceType.TEXT, content=instruction)],
                    is_interrupt=True,
                )
            return None

        # ã‚¤ãƒ™ãƒ³ãƒˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆGUIç”¨ï¼‰
        def event_callback(event_type: str, data: dict):
            if event_type == "speak":
                self._emit_speak_event(
                    run_id,
                    data.get("turn", 0),
                    data.get("speaker", "A"),
                    data.get("text", ""),
                )
                # RAGã‚¤ãƒ™ãƒ³ãƒˆ
                character = self.char_a if data.get("speaker") == "A" else self.char_b
                self._emit_rag_event(
                    run_id,
                    data.get("turn", 0),
                    data.get("speaker", "A"),
                    getattr(character, 'last_rag_hints', []) or [],
                )

        # UnifiedPipeline å®Ÿè¡Œ
        pipeline_result = self.unified_pipeline.run(
            initial_input=bundle,
            max_turns=max_iterations,
            run_id=run_id,
            interrupt_callback=interrupt_callback,
            event_callback=event_callback,
        )

        # çµæœã‚’æ—¢å­˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
        for turn in pipeline_result.dialogue:
            result["dialogue"][f"turn_{turn.turn_number}"] = {
                "speaker": turn.speaker,
                "text": turn.text,
            }

        if pipeline_result.dialogue:
            last_eval = pipeline_result.dialogue[-1].evaluation
            if last_eval:
                result["director_verdict"] = {
                    "status": str(last_eval.status.name),
                    "reason": last_eval.reason,
                    "suggestion": last_eval.suggestion,
                }

        result["status"] = pipeline_result.status
        if pipeline_result.error:
            result["error"] = pipeline_result.error

        # Step 3: ãƒ­ã‚°ã«è¨˜éŒ²
        print("\n[Step 3] Logging to file...")
        log_id = self.logger.log_narration(
            scene_description=scene_description,
            image_path=image_path,
            vision_analysis=result["vision_analysis"],
            dialogue=result["dialogue"],
            director_verdict=result["director_verdict"],
        )
        result["log_id"] = log_id
        print(f"âœ… Logged (ID: {log_id})")

        print(f"\nâœ… Dialogue completed ({len(pipeline_result.dialogue)} turns)")

        return result

    def process_batch(
        self,
        image_list: list,
        output_file: Optional[str] = None,
    ) -> list:
        """
        è¤‡æ•°ã®ç”»åƒã‚’å‡¦ç†ã™ã‚‹ã€‚

        Args:
            image_list: [(image_path, scene_description), ...] ã®ãƒªã‚¹ãƒˆ
            output_file: çµæœã‚’JSONã§å‡ºåŠ›ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆoptionalï¼‰

        Returns:
            çµæœã®ãƒªã‚¹ãƒˆ
        """
        results = []

        for image_path, scene_description in image_list:
            result = self.process_image(
                image_path=image_path,
                scene_description=scene_description,
            )
            results.append(result)

        # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆæŒ‡å®šæ™‚ï¼‰
        if output_file:
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ“ Results saved to: {output_file}")

        return results


def main():
    """
    ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œä¾‹
    """
    pipeline = NarrationPipeline()

    # ãƒ†ã‚¹ãƒˆç”¨ã®ç”»åƒãƒªã‚¹ãƒˆ
    test_images = [
        ("tests/images/temple_sample.jpg", "å¤ã„å¯ºé™¢ã®å¢ƒå†…ã€‚å‚æ‹å®¢ãŒå°‘ãªãã€é™ã‹ãªæ™‚é–“å¸¯ã®ã‚ˆã†ã§ã™ã€‚"),
        ("tests/images/nature_sample.jpg", "ç·‘è±Šã‹ãªå±±é–“ã®é¢¨æ™¯ã€‚è‡ªç„¶ã«åŒ…ã¾ã‚ŒãŸè¦³å…‰åœ°ã§ã™ã€‚"),
    ]

    if not test_images:
        print("No test images provided.")
        print("Usage: python scripts/run_narration.py")
        print("\nTo test:")
        print("1. ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«é…ç½®")
        print("2. ä»¥ä¸‹ã‚’ã‚³ãƒ¼ãƒ‰å†…ã§è¨­å®šï¼š")
        print('   test_images = [("path/to/image.jpg", "ã‚·ãƒ¼ãƒ³èª¬æ˜"), ...]')
        print("3. ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ")
        return

    results = pipeline.process_batch(
        image_list=test_images,
        output_file="runs/narration_results.json",
    )

    print("\n" + "="*60)
    print(f"âœ… Processing complete! ({len([r for r in results if r['status'] == 'success'])} / {len(results)} passed)")
    print("="*60)


if __name__ == "__main__":
    main()
