# LLMバックエンド設定
# Ollama / vLLM (Docker) を切り替えて運用

backends:
  ollama:
    enabled: true
    base_url: "http://localhost:11434/v1"
    api_type: "openai_compatible"
    health_check_url: "http://localhost:11434/api/tags"
    models:
      gemma3-12b:
        name: "gemma3:12b"
        supports_vlm: true
        vram_estimate_gb: 8
        context_length: 8192
        quantization: "Q4_K_M"
        description: "Ollama版 Gemma3 12B"
      gemma3-27b:
        name: "gemma3:27b"
        supports_vlm: true
        vram_estimate_gb: 17
        context_length: 8192
        quantization: "Q4_K_M"
        description: "Ollama版 Gemma3 27B（大型）"

  vllm:
    enabled: true
    base_url: "http://localhost:8000/v1"
    api_type: "openai_compatible"
    health_check_url: "http://localhost:8000/v1/models"
    docker:
      image: "vllm/vllm-openai:latest"
      gpu_memory_utilization: 0.85
      ipc_host: true
      cache_mount: "~/.cache/huggingface:/root/.cache/huggingface"
    models:
      gemma3-27b-int4:
        name: "RedHatAI/gemma-3-27b-it-quantized.w4a16"
        supports_vlm: true
        quantization: "w4a16"
        vram_estimate_gb: 22
        context_length: 4096
        description: "推奨構成（27B INT4量子化）"
        docker_args:
          - "--trust-remote-code"
          - "--max-model-len"
          - "4096"
          - "--dtype"
          - "bfloat16"
          - "--enforce-eager"
      gemma3-12b-int8:
        name: "RedHatAI/gemma-3-12b-it-quantized.w8a8"
        supports_vlm: true
        quantization: "w8a8"
        vram_estimate_gb: 14
        context_length: 8192
        description: "軽量構成（INT8量子化）"
        docker_args:
          - "--trust-remote-code"
          - "--max-model-len"
          - "8192"
      gemma3-12b-gptq:
        name: "ISTA-DASLab/gemma-3-12b-it-GPTQ-4b-128g"
        supports_vlm: true
        quantization: "gptq"
        vram_estimate_gb: 7
        context_length: 8192
        description: "最軽量（GPTQ 4bit）"
        docker_args:
          - "--trust-remote-code"
          - "--max-model-len"
          - "8192"
      qwen25-14b-awq:
        name: "Qwen/Qwen2.5-14B-Instruct-AWQ"
        supports_vlm: false
        quantization: "awq"
        vram_estimate_gb: 10
        context_length: 8192
        description: "テキスト専用（高品質）"
        docker_args:
          - "--quantization"
          - "awq"

# デフォルト設定
defaults:
  backend: "vllm"
  model: "gemma3-12b-int8"
  fallback_backend: "ollama"
  fallback_model: "gemma3-12b"

# Florence-2設定（将来拡張用）
florence2:
  enabled: false
  model: "microsoft/Florence-2-large"
  vram_estimate_gb: 2.5
