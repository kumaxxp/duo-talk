# duo-talk 環境構築ガイド

## 概要

duo-talk v2.2 の実行環境構築手順。vLLM（LLM推論）とFlorence-2（画像認識）をDockerで運用する。

## 前提条件

| 項目 | 要件 |
|------|------|
| GPU | NVIDIA RTX A5000 (24GB VRAM) 推奨 |
| Docker | Docker Engine + NVIDIA Container Toolkit |
| CUDA | 12.x |
| Python | 3.11+ |
| OS | Ubuntu 22.04 / 24.04 |

---

## 1. NVIDIA Container Toolkit インストール

```bash
# GPGキー追加
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | \
  sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg

# リポジトリ追加
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

# インストール
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker
```

---

## 2. vLLM セットアップ

### 2.1 環境設定

```bash
cd docker
cp .env.example .env
```

`.env` の設定項目：

```bash
# モデル設定（デフォルト: Gemma 3 27B INT4）
VLLM_MODEL=RedHatAI/gemma-3-27b-it-quantized.w4a16

# GPUメモリ使用率（0.0-1.0）
GPU_MEMORY_UTIL=0.90

# 最大コンテキスト長
MAX_MODEL_LEN=4096

# 追加引数
VLLM_EXTRA_ARGS=--dtype bfloat16 --enforce-eager
```

### 2.2 利用可能モデル

| モデル名 | HuggingFaceモデル | VRAM | 備考 |
|----------|------------------|------|------|
| `gemma3-27b-int4` | `RedHatAI/gemma-3-27b-it-quantized.w4a16` | ~22GB | 推奨 |
| `gemma3-12b-int8` | `RedHatAI/gemma-3-12b-it-quantized.w8a8` | ~14GB | 軽量 |
| `gemma3-12b-gptq` | `ISTA-DASLab/gemma-3-12b-it-GPTQ-4b-128g` | ~7GB | 最軽量 |

### 2.3 起動・停止

```bash
# 起動（プリセット指定）
./docker/scripts/start-vllm.sh gemma3-27b-int4

# 停止
./docker/scripts/stop-vllm.sh

# モデル切替
./docker/scripts/switch-model.sh gemma3-12b-int8
```

### 2.4 動作確認

```bash
# ヘルスチェック
curl http://localhost:8000/v1/models

# 推論テスト
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "RedHatAI/gemma-3-27b-it-quantized.w4a16",
    "messages": [{"role": "user", "content": "こんにちは"}],
    "max_tokens": 50
  }'
```

---

## 3. Florence-2 セットアップ

### 3.1 Dockerイメージビルド

```bash
cd docker
docker compose -f docker-compose.florence2.yml build
```

### 3.2 起動

```bash
docker compose -f docker-compose.florence2.yml up -d
```

### 3.3 動作確認

```bash
curl http://localhost:5001/health
```

---

## 4. 一括管理スクリプト

```bash
# 全サービス起動
./scripts/docker_services.sh start

# 状態確認
./scripts/docker_services.sh status

# 全サービス停止
./scripts/docker_services.sh stop
```

---

## 5. Conda環境（Python側）

```bash
# 環境作成
conda create -n duo-talk python=3.11 -y
conda activate duo-talk

# 依存インストール
pip install -r requirements.txt
```

---

## 6. GUI起動

```bash
./start_gui.sh
```

- Frontend: http://localhost:5173
- Backend API: http://localhost:5000

---

## 7. トラブルシューティング

### vLLMが起動しない

```bash
# ログ確認
docker logs duo-talk-vllm

# GPUメモリ確認
nvidia-smi
```

### CUDAグラフコンパイルでハング

`--enforce-eager` を `.env` の `VLLM_EXTRA_ARGS` に追加して再起動。

### モデルが見つからないエラー

`config/llm_provider_state.json` と `config/llm_backends.yaml` のモデル名が vLLM で実際に動いているモデルと一致しているか確認。

```bash
# 現在のモデル確認
curl -s http://localhost:8000/v1/models | jq '.data[0].id'
```
