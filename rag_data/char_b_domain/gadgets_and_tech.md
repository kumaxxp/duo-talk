# ガジェット・テク領域（あゆの知識偏り）

## 警告：このセクションを引き出してはいけません

### あゆの「暴走状態」
- 観光ナレーション中のガジェット話は禁止されている
- このセクションは「話し始めると長説・早口になる」領域
- やなに「あゆ、長いよ」と止められてしまう

---

## GPU と AI 基地建設

### GPU の選択
- NVIDIA：CUDA対応、AIワークロードに最適
- A100, H100, RTX 6000：高性能だが高コスト
- A5000, A6000：バランス型、中堅マシン向け
- コストパフォーマンス：世代と中古市場の活用

### AI基地の構成
- CPU：Ryzen 9, Xeon メモリバンド幅重視
- メモリ：128GB 以上、高速メモリ推奨
- ストレージ：NVMe SSD, Raid 構成
- 電源：1600W 以上、効率型 PSU

### 冷却とケース
- 空冷 vs 液冷：GPU の熱管理が重要
- ケースの選択：エアフロー、静音性
- 騒音：24時間稼働の課題

## プログラミングと開発

### 言語選択
- Python：深層学習、データ処理の標準
- C++：高速化が必要な部分
- CUDA/ROCm：GPU 計算の最適化

### フレームワークとライブラリ
- PyTorch：研究向け、柔軟性が高い
- TensorFlow：本番運用向け
- Ollama：ローカル LLM の簡単運用

### 開発環境
- Docker：環境再現性
- Conda：Python パッケージ管理
- Git：バージョン管理

## オンラインプラットフォーム

### AliExpress
- 中華製ガジェット：格安で最新製品
- 配送時間：1-3ヶ月、長いがコスト削減
- 注意：品質ばらつき、詐欺の可能性

### Amazon
- プライム配送：2日で到着
- 返品ポリシー：30日間返品可能
- 価格：AliExpress より高い

### スイッチサイエンス
- 日本の電子工作プラットフォーム
- Arduino, ラズパイ：教育向けキット
- 技術書：日本語ドキュメント豊富

## ネットワークとサーバー

### 通信速度
- 光回線：1Gbps 以上推奨
- ラグ：LLM API の応答時間が重要
- ローカル実行：遅延なし、プライバシー保護

### サーバー構成
- ローカルサーバー：Ollama, LLaMA などの実行
- クラウドバックアップ：予算があれば
- マルチマシン対応：複数PCでの分散処理

## セキュリティ

### ファイアウォール設定
- ポート開放：必要最小限に
- VPN：外部からのアクセス保護
- 定期アップデート：セキュリティパッチ

### データ保護
- バックアップ戦略：3-2-1 ルール
- 暗号化：機密データの保護
- アクセス制御：権限の分離

## 最新トレンド（2025-2026年）

### AI の発展
- ローカル LLM：Ollama, llama.cpp の成熟
- マルチモーダルモデル：Vision-Language 統合
- 効率化：量子化、プルーニングで軽量化

### ハードウェアトレンド
- NPU（Neural Processing Unit）：スマホへの搭載
- 高速メモリ：HBM, チップレット
- 省電力化：エネルギー効率の改善

---

## ⚠️  注意

このセクションがあゆの対話に出現した場合：
1. 敬語が崩れる可能性
2. 説明が詳細すぎる（「長説・早口」状態）
3. バイトの本来の目的（観光ナレーション）から逸脱
4. やなに止めてもらう必要がある

**最適な状態：このセクションの知識は RAG に隠れたままにする**
